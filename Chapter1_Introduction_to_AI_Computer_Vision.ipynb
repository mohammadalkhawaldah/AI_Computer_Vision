{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 1 ‚Äî Introduction to AI Computer Vision (Mechatronics Systems View)\n",
        "**Course:** Special Topics in Mechatronics Engineering (AI + Computer Vision + Intelligent Controllers)  \n",
        "**Instructor:** Dr. Mohammad Al Khawaldah  \n",
        "**Notebook type:** Lecture + Lab (interactive)  \n",
        "**Last updated:** 2026-02-16\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Outcomes\n",
        "By the end of Chapter 1, you will be able to:\n",
        "\n",
        "1. Explain the full **Vision‚ÜíDecision‚ÜíController** pipeline (system engineering, not model-only).\n",
        "2. Define core terms precisely: **model, training, inference, dataset, labels, classes, confidence, threshold, NMS, tracking, domain shift, latency**.\n",
        "3. Run a complete example application (frame ‚Üí inference ‚Üí decision gate ‚Üí ‚Äúactuator command‚Äù + evidence logging).\n",
        "4. Ask the right **engineering questions** and identify common **failure modes** in industrial vision systems.\n",
        "\n",
        "> In mechatronics, the model is only a part of the system. The rest is engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1) Introduction: AI Computer Vision in Mechatronics\n",
        "\n",
        "Computer Vision transforms raw pixels into structured understanding (objects, states, defects, motion).  \n",
        "Mechatronics turns that understanding into **actions** that affect the physical world.\n",
        "\n",
        "### Why it is different from ‚Äúpure AI‚Äù\n",
        "A real mechatronics vision system must satisfy:\n",
        "- **Latency constraints** (real-time control, moving targets, conveyor speed)\n",
        "- **Safety constraints** (false alarms create ‚Äúalarm fatigue‚Äù; missed detections can cause accidents)\n",
        "- **Hardware constraints** (CPU/GPU, memory, power, edge vs cloud)\n",
        "- **Environmental variation** (lighting, dust, glare, camera shift) ‚Üí domain shift\n",
        "\n",
        "We do not just train models ‚Äî we design **end-to-end systems**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2) The Complete AI Vision Pipeline (End-to-End System)\n",
        "\n",
        "Canonical pipeline in industrial mechatronics:\n",
        "\n",
        "**Sensor ‚Üí Preprocessing ‚Üí Model Inference ‚Üí Post-processing ‚Üí Decision Logic ‚Üí Controller Interface ‚Üí Actuator ‚Üí Logging**\n",
        "\n",
        "You must be able to explain what happens at each step, and what can go wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 2.1 Flowchart diagram (auto-generated) using Graphviz\n",
        "# =========================================================\n",
        "!pip -q install graphviz\n",
        "\n",
        "from graphviz import Digraph\n",
        "\n",
        "g = Digraph(\"vision_pipeline\", format=\"png\")\n",
        "g.attr(rankdir=\"LR\", fontsize=\"12\")\n",
        "\n",
        "g.node(\"S\", \"Sensor\\n(Camera/Stream)\", shape=\"box\")\n",
        "g.node(\"P\", \"Preprocessing\\n(resize, normalize, ROI)\", shape=\"box\")\n",
        "g.node(\"M\", \"Model Inference\\n(CLS/DET/SEG)\", shape=\"box\")\n",
        "g.node(\"PP\", \"Post-processing\\n(threshold, NMS, tracking)\", shape=\"box\")\n",
        "g.node(\"D\", \"Decision Logic\\n(state machine, safety gate)\", shape=\"box\")\n",
        "g.node(\"C\", \"Controller Interface\\n(serial/MQTT/REST/ROS)\", shape=\"box\")\n",
        "g.node(\"A\", \"Actuator\\n(LED/buzzer/gate/motor)\", shape=\"box\")\n",
        "g.node(\"L\", \"Logging\\n(evidence + metrics)\", shape=\"box\")\n",
        "\n",
        "g.edges([(\"S\",\"P\"), (\"P\",\"M\"), (\"M\",\"PP\"), (\"PP\",\"D\"), (\"D\",\"C\"), (\"C\",\"A\"), (\"D\",\"L\")])\n",
        "\n",
        "g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Engineering interpretation\n",
        "- Preprocessing changes both accuracy and latency.\n",
        "- Post-processing prevents duplicated boxes (NMS) and stabilizes decisions (temporal smoothing).\n",
        "- Decision logic is where safety policies live (e.g., ‚Äú3 consecutive frames required‚Äù).\n",
        "- Logging is mandatory for audits and debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3) Industrial Applications (Detailed)\n",
        "\n",
        "## A) Truck Load Compliance Inspection\n",
        "**Goal:** Detect whether a truck bed is covered/uncovered/irregular and optionally classify visible load type.\n",
        "\n",
        "**Inputs:** fixed CCTV stream (RTSP). Sometimes a separate ANPR camera provides plate number as API.  \n",
        "**Outputs:** violation/no-violation event + evidence snapshot + optional actuator (gate/buzzer).\n",
        "\n",
        "**System steps:**\n",
        "1. Detect truck/bed region (DET) ‚Üí crop ROI for better accuracy.\n",
        "2. Classify cover state (CLS) OR segment cover/exposed regions (SEG).\n",
        "3. Temporal gate: stable violation for N frames to avoid ‚Äúone-frame noise‚Äù.\n",
        "4. Merge repeated detections into one event per truck using tracking ID.\n",
        "5. Log timestamp, confidence, snapshot, and plate ID (fusion).\n",
        "\n",
        "**Hard real-world conditions:**\n",
        "glare, shadows, night IR, motion blur, camera shift (domain shift), multiple trucks, occlusions.\n",
        "\n",
        "## B) PPE Safety (Helmet detection)\n",
        "Detect person + helmet; alarm if person without helmet persists >2 seconds (avoid alarm spam).\n",
        "\n",
        "## C) Conveyor Quality Inspection\n",
        "Detect defects/missing parts and trigger reject actuator (air jet/robot arm). Strict latency.\n",
        "\n",
        "## D) Drone Vision\n",
        "Detect targets from moving camera. Bandwidth + motion blur constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 4) Definitions (Core Terms)\n",
        "\n",
        "### Model\n",
        "A model is a function **fŒ∏(x)** mapping input image x ‚Üí output y. Œ∏ are learned parameters.\n",
        "\n",
        "### Training vs Inference\n",
        "- Training: learn Œ∏ from labeled dataset by minimizing a loss.\n",
        "- Inference: run trained model on new frames ‚Üí predictions.\n",
        "\n",
        "### Dataset, Labels, Classes\n",
        "- Dataset: samples (images/frames) + labels.\n",
        "- Label/Annotation: ground truth (class / boxes / masks).\n",
        "- Class: category name (‚Äúcovered‚Äù, ‚Äúuncovered‚Äù, ‚Äúhelmet‚Äù, ‚Ä¶).\n",
        "\n",
        "### Confidence & Threshold\n",
        "- Confidence: model score for prediction (not always calibrated probability).\n",
        "- Threshold: accept prediction only if score ‚â• threshold.\n",
        "\n",
        "### NMS\n",
        "Removes duplicated overlapping boxes (detection).\n",
        "\n",
        "### Tracking\n",
        "Persistent object IDs across frames; prevents multiple events per same object.\n",
        "\n",
        "### Overfitting\n",
        "Train ‚Üë, val ‚Üì. Model memorizes.\n",
        "\n",
        "### Domain shift\n",
        "Training distribution ‚â† deployment distribution (angle/lighting/weather). #1 industrial failure.\n",
        "\n",
        "### Latency\n",
        "Capture ‚Üí decision time. Must meet real-time needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 5) Complete Example Application (End-to-End)\n",
        "## ‚ÄúUncovered Truck‚Äù ‚Üí Event + Buzzer Command + Evidence\n",
        "\n",
        "We will run a full industrial-style loop:\n",
        "1. Read frames (here: images downloaded automatically).\n",
        "2. Simulate model inference (toy classifier for teaching pipeline).\n",
        "3. Apply threshold and temporal gate (N consecutive detections).\n",
        "4. Trigger ‚Äúactuator command‚Äù (print command).\n",
        "5. Log evidence (save snapshot + metadata).\n",
        "\n",
        "Later chapters replace toy model with YOLO/real models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Auto-download example images (no manual copy/paste)\n",
        "This cell downloads images automatically into the runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 5.1 Download example images automatically (public-domain sources)\n",
        "# =========================================================\n",
        "import os, requests\n",
        "\n",
        "os.makedirs(\"chapter1_images\", exist_ok=True)\n",
        "\n",
        "image_urls = {\n",
        "  \"dump_truck\": \"https://upload.wikimedia.org/wikipedia/commons/3/3f/Dump_truck.jpg\",\n",
        "  \"construction_waste\": \"https://upload.wikimedia.org/wikipedia/commons/1/14/Construction_and_demolition_waste.jpg\",\n",
        "  \"safety_helmet\": \"https://upload.wikimedia.org/wikipedia/commons/5/5d/Hard_hat.jpg\"\n",
        "}\n",
        "\n",
        "for name, url in image_urls.items():\n",
        "    out = f\"chapter1_images/{name}.jpg\"\n",
        "    if not os.path.exists(out):\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        with open(out, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "        print(\"Downloaded:\", out)\n",
        "    else:\n",
        "        print(\"Exists:\", out)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 5.2 Visualize images\n",
        "# =========================================================\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show(path, title=None):\n",
        "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title or os.path.basename(path))\n",
        "    plt.show()\n",
        "\n",
        "for p in sorted(os.listdir(\"chapter1_images\")):\n",
        "    show(os.path.join(\"chapter1_images\", p), title=p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 A toy ‚Äúmodel‚Äù (teaching device)\n",
        "We compute a simple texture score using Laplacian variance and map it to [0,1].\n",
        "This illustrates: model input ‚Üí score ‚Üí threshold decision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def toy_uncovered_score(rgb_img: np.ndarray) -> float:\n",
        "    gray = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2GRAY)\n",
        "    lap = cv2.Laplacian(gray, cv2.CV_64F)\n",
        "    texture = lap.var()\n",
        "    score = 1 / (1 + np.exp(-(texture - 200) / 80))\n",
        "    return float(score)\n",
        "\n",
        "def predict_score(image_path: str) -> float:\n",
        "    rgb = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "    return toy_uncovered_score(rgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 Temporal gate (safety rule)\n",
        "Never trigger from a single frame. Require N consecutive frames above threshold + cooldown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TriggerPolicy:\n",
        "    threshold: float = 0.55\n",
        "    min_consecutive: int = 2\n",
        "    cooldown_s: float = 1.0\n",
        "\n",
        "class TemporalGate:\n",
        "    def __init__(self, policy: TriggerPolicy):\n",
        "        self.p = policy\n",
        "        self.consec = 0\n",
        "        self.last_fire = 0.0\n",
        "\n",
        "    def update(self, score: float) -> bool:\n",
        "        now = time.time()\n",
        "        if now - self.last_fire < self.p.cooldown_s:\n",
        "            self.consec = 0\n",
        "            return False\n",
        "\n",
        "        if score >= self.p.threshold:\n",
        "            self.consec += 1\n",
        "        else:\n",
        "            self.consec = 0\n",
        "\n",
        "        if self.consec >= self.p.min_consecutive:\n",
        "            self.last_fire = now\n",
        "            self.consec = 0\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 Controller command + evidence logging\n",
        "We simulate Arduino/PLC command and save evidence snapshot + JSON metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json, shutil\n",
        "from datetime import datetime\n",
        "\n",
        "os.makedirs(\"evidence\", exist_ok=True)\n",
        "\n",
        "def send_controller_command(cmd: str):\n",
        "    print(f\"[CONTROLLER] {cmd}\")\n",
        "\n",
        "def log_evidence(image_path: str, score: float, reason: str):\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out_img = f\"evidence/{ts}_{os.path.basename(image_path)}\"\n",
        "    shutil.copy(image_path, out_img)\n",
        "    meta = {\"timestamp\": ts, \"image\": out_img, \"score\": score, \"reason\": reason}\n",
        "    with open(out_img + \".json\", \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(\"[LOGGED]\", meta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.6 Run the end-to-end demo loop\n",
        "We loop over ‚Äúframes‚Äù (images) and apply the system logic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "policy = TriggerPolicy(threshold=0.55, min_consecutive=2, cooldown_s=1.0)\n",
        "gate = TemporalGate(policy)\n",
        "\n",
        "frame_paths = [\n",
        "    \"chapter1_images/dump_truck.jpg\",\n",
        "    \"chapter1_images/construction_waste.jpg\",\n",
        "    \"chapter1_images/dump_truck.jpg\",\n",
        "    \"chapter1_images/construction_waste.jpg\",\n",
        "]\n",
        "\n",
        "print(\"Policy:\", policy)\n",
        "\n",
        "for i, fp in enumerate(frame_paths):\n",
        "    score = predict_score(fp)\n",
        "    fire = gate.update(score)\n",
        "    print(f\"Frame {i:02d}: {os.path.basename(fp):25s} score={score:.3f} fire={fire}\")\n",
        "    if fire:\n",
        "        send_controller_command(\"BUZZER_ON\")\n",
        "        log_evidence(fp, score, reason=f\"score>=threshold for {policy.min_consecutive} consecutive frames\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚úÖ You executed a full industrial-style pipeline:\n",
        "frames ‚Üí model score ‚Üí threshold + temporal smoothing ‚Üí actuator command ‚Üí evidence logging.\n",
        "\n",
        "Later chapters will replace the toy model with YOLO models trained on your datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6) Key Questions (Engineering Checklist)\n",
        "\n",
        "### System questions\n",
        "1. Edge vs cloud inference? What if network fails?\n",
        "2. End-to-end latency budget? Required FPS?\n",
        "3. Cost of false positives vs false negatives?\n",
        "4. Tracking needed to enforce ‚Äúone event per object‚Äù?\n",
        "5. What evidence must be stored (snapshot + confidence + reason)?\n",
        "\n",
        "### Data questions\n",
        "1. Does dataset match deployment camera angle/lighting?\n",
        "2. Is dataset balanced?\n",
        "3. Are labels consistent?\n",
        "4. Do we have rare critical cases (night/rain/occlusion)?\n",
        "\n",
        "### Model questions\n",
        "1. Need classification/detection/segmentation?\n",
        "2. Smallest model meeting accuracy+speed?\n",
        "3. Which threshold matches safety policy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 7) Failure Modes (Real-World)\n",
        "\n",
        "### Data failures\n",
        "Label noise, dataset bias (day-only), leakage, missing corner cases.\n",
        "\n",
        "### Model failures\n",
        "Overfitting, slow model (latency), poor confidence calibration.\n",
        "\n",
        "### Deployment failures\n",
        "Camera moved, dirty lens, compression artifacts, new unseen objects (domain drift).\n",
        "\n",
        "### System logic failures\n",
        "No temporal smoothing, no cooldown, no tracking, no logging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 8) Exercises (Graded)\n",
        "\n",
        "## Exercise 1 ‚Äî Threshold trade-off\n",
        "Change `policy.threshold` to 0.80 and rerun. Explain what happens (FP vs FN).\n",
        "\n",
        "## Exercise 2 ‚Äî Stronger temporal gating\n",
        "Set `min_consecutive` to 3. Add more frames to `frame_paths`. Observe stability.\n",
        "\n",
        "## Exercise 3 ‚Äî Evidence\n",
        "Open the `evidence/` folder and inspect saved files. Why is this mandatory in compliance systems?\n",
        "\n",
        "## Exercise 4 ‚Äî Design (written)\n",
        "Pick one application (truck/PPE/conveyor/drone) and write:\n",
        "- pipeline blocks\n",
        "- where domain shift can happen\n",
        "- what you will log as evidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ‚úÖ Chapter 1 Summary\n",
        "You learned:\n",
        "- AI vision is a **system**, not only a model.\n",
        "- Key terms and how they connect in the pipeline.\n",
        "- A complete end-to-end demo with safety gating and evidence logging.\n",
        "- Engineering questions + failure modes.\n",
        "\n",
        "üëâ Next: Module 2 ‚Äî Image Classification (dataset, training, evaluation, deployment)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Chapter1_Introduction_to_AI_Computer_Vision.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}